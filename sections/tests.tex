\chapter{Tests}

\textbf{Author: Timon Koch}

\section{Specialized Benchmarks and Tests}
\subsection{Serialization and Compression}

\section{Optimization}
\subsection{Compression}
\textbf{Timon Koch}
By reducing the size of data, compression saves storage space, speeds up data transmission over networks and minimises bandwidth requirements, resulting in cost savings and improved operational efficiency. In addition, compressed data is easier to manage, share and access across platforms and devices. 

\subsubsection{Deflate}
The Deflate algorithm is a widely used method of lossless data compression. It achieves compression by eliminating redundancy in the input data through a combination of LZ77 and Huffman coding techniques, making it an efficient and versatile compression method used in various applications.

In the first step, LZ77 identifies repeated substrings in the data and replaces them with references to previous occurrences, thereby reducing redundancy. This is followed by Huffman coding, which assigns shorter codes to more frequent symbols and longer codes to less frequent symbols, further compressing the data

\subsubsection{ZSTD}
ZSTD, short for Zstandard, is a high performance data compression algorithm developed by Facebook. It is designed to provide both fast compression and decompression speeds while achieving excellent compression ratios, making it suitable for a wide range of applications.

ZSTD uses a combination of advanced compression techniques, including a dictionary-based approach, context modelling and entropy coding. It dynamically builds and updates dictionaries during compression to improve compression efficiency by identifying and exploiting patterns in the data.

\subsubsection{GZIP}
GZIP, short for GNU Zip, is based on the Deflate algorithm. While it's optimised for speed, GZIP also provides reasonably good compression ratios, making it a popular choice for applications where both speed and compression efficiency are important.

GZIP typically offers several levels of compression, allowing users to trade off compression ratio and speed according to their needs. Lower compression levels prioritise faster processing times, while higher levels prioritise better compression ratios at the expense of speed.

\subsection{Buffering}

\subsection{Serialization Formats}
\textbf{Author: Timon Koch}

\subsubsection{JSON}
JSON (JavaScript Object Notation) is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is based on a subset of the JavaScript programming language.

JSON consists of key-value pairs, where keys are strings and values can be strings, numbers, arrays, objects, booleans, or null. It is highly versatile and widely supported across various programming languages and platforms.

\subsubsection{YAML}
YAML (YAML Ain't Markup Language) is a data serialisation format commonly used for configuration files, data exchange, and specifying data structures in a clear, concise, and readable manner. It uses indentation and whitespace to represent data hierarchies, making it visually intuitive and easy to understand.

YAML supports multiple data types, including scalars (strings, numbers, booleans), sequences (arrays or lists), and mappings (key-value pairs). It also allows for comments, making it useful for annotating configurations and documentation.

\subsubsection{Protobuf}
Protocol Buffers (protobuf) is a method of serialising structured data developed by Google. Designed to be efficient, portable and easy to use, it provides a platform-neutral way to encode data for communication between systems or for persistent storage.

Protobuf defines a schema for data structures using a language-agnostic interface description language (IDL). This schema describes the structure of the data in a concise and platform-independent manner. From this schema, Protobuf compilers generate code in various programming languages, allowing developers to easily work with the defined data structures.

\subsubsection{Bincode}
Bincode is a binary serialisation format designed for use with the Rust programming language. It allows the serialisation of Rust data structures into a compact binary representation that can be efficiently stored or transmitted. Bincode is designed to be fast and efficient, making it suitable for scenarios where performance is critical, such as networking, file I/O or inter-process communication.

\subsubsection{Rust UTF-8}
Rust UTF-8 is a variable-width encoding that can represent any Unicode code point using one to four bytes.

In Rust, strings are encoded as UTF-8 by default. This means that Rust provides native support for manipulating, validating, and converting UTF-8 encoded strings efficiently and securely. Rust's type system ensures that operations on strings are performed in a way that preserves the integrity of the UTF-8 encoding, preventing common problems such as invalid byte sequences or buffer overflows.


\tocdata{toc}{$\rightarrow$\textit{Christoph Fellner}}
\subsection{Database Benchmarks}
\textbf{Author: Christoph Fellner}

RECT uses a register to store data about available connections and useable ports of other controllers. This register is stored in a in-memory database. In order to find the 
best database solution for our use case, we compare SQLite, PostgreSQL and MySQL. Using docker as testing environment we can easily compare the different databases.\newline

In order to test the databases without any influences from outside, we used docker container for each database and their corresponding client program. The client program
is written in rust and uses the corresponding rust librarys for the individual databases. However since SQLite is a file based database, we just used a volume to store the
database file, together with the rust program in one container. The rust program connects to the database and executes the queries. The program measures the time it 
takes to execute the query and prints it to the console. Because we are using docker, we can easily track the resource usage of the database container during the query.\newline

The test setup is as follows: We use four tables with 100 rows and 5 columns. The columns are of type integer and string and the rows are filled with random data. We then 
execute a simple select query with join statements to get all the data from the table. We measure the time it takes to execute the query and the resource usage of the database 
container. We repeat this process 50 times and calculate the average time it takes to execute the query.\newline

The results are as follows:
\begin{center}
    \begin{tabular}{ | m{3cm} | m{4cm}| m{4cm} | m{4cm} | } 
      \hline
      Database & avg time to execute & avg CPU usage & avg Memory usage \\ 
      \hline
      SQLite & 95,86ms & 15,67\% & 54,97MB \\ 
      \hline
      PostgreSQL & 105,52ms & 17,32\% & 58,86MB \\ 
      \hline
      MySQL & 109,63ms & 18,51\% & 59,12MB \\
      \hline
    \end{tabular}
\end{center}

The results show that SQLite is the fastest database, and also uses the least amount of resources. Additionally, SQLite was the easiest to set up and use. So we decided to use
SQLite as the database for the register in RECT.

\filbreak