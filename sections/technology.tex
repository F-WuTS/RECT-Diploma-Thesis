\chapter{Technology}


\section{Wombat}

\section{Python}

\section{C++}

\section{Rust}
\textbf{Author: Jeremy Sztavinovszki}

Rust is a general purpose multi paradigm programming language used in many fields ranging from embedded programming to web development. Although it is a relatively young language, having released its version 1.0 on May 15th 2015, it has seen great adoption from developers and has a big community. The language tries to be as fast as possibly, while still remaining memory-safe, which it achieves using its borrow checker. Even though it is possible to write unsafe code in Rust, that is not checked by the borrow checker, it is custom to keep the unsafe parts as small as possible.
Rust has a great ecosystem driven by the Rust Foundation and the Rust community. There are many tools, such as cargo, or rust-gdb, that provide great developer experience.
Right now there is now standardized async-runtime, so you normally use runtimes like tokio, async-std, or smol for programming asynchronously.

\subsection{Cargo}
\textbf{Author: Christoph Fellner}

Cargo\footcite{cargo} is a build system and package manager for Rust, providing developers with a robust toolset for managing Rust projects. A rust package consists of a
\verb+Cargo.toml+ file, which contains metadata about the package, and a \verb+src+ folder, which contains the source code as rust-files (\verb+.rs+). It is an integral 
part of the Rust ecosystem and plays a crucial role in managing dependencies, building projects, and facilitating a smooth development workflow.

Here's an overview of Cargo's main functionalities:

\begin{itemize}
    \item 1. Dependency Management: Cargo manages project dependencies by automatically fetching and incorporating external libraries or crates. Developers specify 
        dependencies in the Cargo.toml file, and Cargo ensures the correct versions are used.
    \item 2. Project Structure: Cargo establishes conventions for organizing Rust projects, defining a standardized layout for directories and files. This consistency 
        helps developers understand and contribute to projects more easily.
    \item 3. Building and Compilation: Cargo handles the complexities of building and compiling Rust projects. Developers can use simple commands such as \verb+'cargo build'+ to 
        compile the code and \verb+'cargo run'+ to execute the compiled binary.
    \item 4. Testing: Cargo supports the integration of unit tests into Rust projects. Developers can use the \verb+'cargo test'+ command to run test suites, ensuring the 
        reliability and correctness of their code.
    \item 5. Documentation Generation: Cargo can generate documentation for Rust projects, making it easier for developers to create and maintain documentation for their 
        code. The \verb+'cargo doc'+ command generates and hosts documentation, and it can be published on platforms like docs.rs.
    \item 6. Project Initialization: Cargo provides a convenient way to initialize new Rust projects with the \verb+'cargo new'+ command. This command sets up a new project with 
        the necessary directory structure and initial files.
    \item 7. Publishing: Cargo facilitates the process of publishing Rust packages (crates) to the official package registry, crates.io. This makes it straightforward for 
        developers to share their libraries and projects with the wider Rust community.
\end{itemize}

In essence, Cargo streamlines various aspects of Rust development, offering a standardized and efficient workflow for managing dependencies, building projects, testing 
code, generating documentation, and publishing packages. Its integration with the Rust toolchain contributes to the language's reputation for being developer-friendly and 
conducive to building robust and reliable systems.

\section{gRPC}

\section{Bluetooth Low Energy}
\textbf{Author: Jeremy Sztavinovszki}

Bluetooth Low Energy (BLE) is a low powered, low cost, low bandwidth radio communication technology, that was originally developed at Nokia in a project named Wibree. It was later noticed by the Bluetooth Special interest group and became part of the Bluetooth 4.0 Core Specification. Nowadays it is often used in all things ranging from wireless headphones to IOT devices and has seen great adoption in many different areas. The BLE protocol Stack, like others (e.g. TCP/IP) is separated into different layers. The layers are split into 3 overarching layers. which are the Application, Host and Controller layers.

\subsection{Application-Layer}
Much like the TCP/IP-Stack the BLE-Stack also comes with an Application-Layer. The Application-Layer is the highest layer in the stack and is responsible for containing logic, user interface and handling the data of the application using BLE. It often determines which usage model is used in the Host-Layers.
It is the layer that bundles all of the functionality of BLE together and abstracts it in such a way, that it is useable for ordinary users.

\subsection{Host-Layer}
The Host-Layer itself splits off into several layers. It is made up of all layers above the Host Side HCI, except the Application Layer, but not only that. It also contains profiles, which
determine how the protocols in the host layer should work with oneanother depending on the usage model, that has been chosen by the Application.

\subsubsection{Generic Access Profile GAP}
The Generic Access Profile, or GAP defines which role a BLE-Device has in communication. These roles determine how the device, as well as other devices act, when sending or receiving data.
A BLE device can take on 4 distinct roles, which are as follows.

\begin{itemize}
\item{Broadcaster}
\item{Observer}
\item{Peripheral}
\item{Central}
\end{itemize}


\subsubsection{Generic Attribute Profile GATT}
GATT

\subsubsection{Logical Link Protocol and Adaptation Protocol L2CAP}
L2CAP

\subsubsection{Attribute Protocol ATT}
ATT

\subsubsection{Security Manager Protocol SMP}
test

\subsubsection{Host Controller Interface HCI (Host side)}
test

\subsection{Controller}
The controller is the layer works closely with the hardware. It contains the following layers

\subsubsection{Host Controller Interface (Controller side)}
test

\subsubsection{Link Layer LL}
Hidden behind the HCI is the Link Layer. It is usually implemented as a conglomeration of custom hardware, as well as software it is the only part
of the protocol stack, that needs to have real-time capabilities, because it needs to work with the timing requirements defined by the specification.
In order to avoid overloading the CPU running the software layers of the stack, the easily automated, or computationally expensive parts are implemented in circuitry.


\subsubsection{Physical Layer PHY}
The physical layer is made up of the actual hardware, that is capable of modulating and demodulating the analog signals sent by radio and turning them into digital information. It uses the 2.4GHz ISM \footcite{ism} radio band, which it splits into 40 channels (37 for transmitting data and 3 for advertising connections and broadcasts) from 2.4GHz to 2.4835GHz. It uses FHSS to avoid radio interference, which is important, because classic bluetooth, as well as 2.4GHz use the same frequency band. The modulation rate chosen for BLE is 1.0Mbit/s, which means that is the physical throughput limit. However because of protocol overhead this physical throughput level is never reached.

\section{Libraries}

\subsection{Rusqlite}
\textbf{Author: Christoph Fellner}

Rusqlite\footcite{rusqlite} emerges as a pivotal library, facilitating the seamless integration of SQLite capabilities into Rust code, akin to its analog, 
rust-postgres. This library provides a sophisticated interface, streamlining the execution of diverse database operations within the Rust programming paradigm. 
From the orchestration of table creation and data insertion to the intricacies of data querying, rusqlite empowers developers with a versatile toolset for 
database manipulation directly within their Rust codebase.

The decision to adopt rusqlite in our specific use case is underpinned by a meticulous consideration of three critical factors:

\begin{enumerate}
    \item \textbf{Portability:} SQLite's commendable attribute of cross-platform compatibility across diverse operating systems assumes paramount importance in 
    our context at RECT. Given the heterogeneous nature of small controllers operating within our system, the ability to seamlessly deploy SQLite databases 
    across different platforms becomes imperative for maintaining operational uniformity and efficiency.
    
    \item \textbf{Configuration:} In stark contrast to database systems that necessitate intricate setup procedures, SQLite obviates the need for complex 
    configurations. The simplicity inherent in working with SQLite aligns seamlessly with our operational requirements. The nominal requirement for a limited 
    number of tables further amplifies the pragmatic appeal of SQLite. In contrast to configuring a database on each controller, the minimal setup overhead 
    consolidates SQLite as the optimal choice for our streamlined operational demands.
    
    \item \textbf{Local:} SQLite distinguishes itself by eschewing the necessity for an external server or intricate installations. Its self-contained package 
    encapsulates all requisite features within a local environment. This intrinsic localization resonates with the operational ethos at RECT, aligning with the 
    need for an efficient and autonomous database solution without the encumbrance of external dependencies.
\end{enumerate}

In our specific instantiation, rusqlite is strategically employed for the persistent storage of the config.json file, a repository of vital data concerning 
available connections. The judicious selection of rusqlite in this context is grounded in its inherent advantages. The utilization of rusqlite markedly enhances 
the expediency and security of data access compared to traditional file-based retrieval mechanisms. This methodological choice contributes not only to the 
enhanced performance of our system but also underscores the commitment to preserving the integrity and reliability of configuration data, pivotal to the seamless
functionality of our application within the broader scientific and technological landscape.

\subsection{Serde}
\textbf{Author: Christoph Fellner}

Serde\footcite{serde} emerges as a pivotal tool in the Rust programming ecosystem, dedicated to the efficient and generic serialization and deserialization 
(\textbf{ser}ialization/\textbf{de}serialization) of data structures. Offering a robust foundation for these operations, Serde plays a crucial role in optimizing
the handling of data structures within the Rust programming paradigm. For a comprehensive overview of Serde, interested readers can refer to the detailed 
documentation available at \href{https://serde.rs/}{https://serde.rs/}.

The salient features of Serde become particularly evident in its capability to facilitate the seamless deserialization of JSON files with efficiency and 
simplicity. This functionality proves invaluable in our context, where the utilization of data from the config.json file is an integral part of our program. 
Serde streamlines this process, enabling us to incorporate the data effortlessly into our program with just a few lines of code.

By leveraging Serde, we engage in a streamlined deserialization process wherein the data from the JSON file is efficiently transformed into a custom Rust 
structure. This bespoke structure, tailored to our specific needs, enables us to harness the deserialized data seamlessly within our program. The utilization of
Serde thus transcends mere deserialization, providing us with a powerful and adaptable mechanism to work with the data in a meaningful and efficient manner.

In essence, the incorporation of Serde into our workflow is not merely a technical choice but a strategic one, driven by the need for optimized data handling 
and seamless integration of external data sources. Through Serde, our program gains a robust and flexible capability to decode JSON files, thereby enhancing the
overall efficiency and maintainability of our Rust-based application.

\subsection{Tokio}
\textbf{Author: Christoph Fellner}

Tokio\footcite{tokio} serves as a pivotal asynchronous runtime for Rust, addressing the intricacies of asynchronous code execution in the language. In Rust, 
asynchronous code does not inherently execute independently; rather, it necessitates the use of a runtime like Tokio to effectively function. For an in-depth 
exploration of Tokio and its capabilities, readers are encouraged to delve into the detailed tutorial available at 
\href{https://tokio.rs/tokio/tutorial}{https://tokio.rs/tokio/tutorial}.

The selection of Tokio as our asynchronous runtime is underpinned by several compelling reasons. Chief among them is Tokio's status as the preeminent and widely
adopted runtime for asynchronous Rust code. Its widespread usage within the Rust community attests to its robustness and reliability in facilitating asynchronous
programming paradigms.

Moreover, Tokio's popularity is bolstered by the abundance of tutorials and educational resources available, making it approachable for developers seeking to 
harness the power of asynchronous programming in Rust. The simplicity and accessibility of Tokio's interface contribute to its widespread adoption, enabling 
developers to swiftly grasp and implement asynchronous patterns in their codebase.

One of the key advantages of Tokio lies in its ability to execute multi-threaded asynchronous code safely. This capability is crucial in scenarios where 
parallelism and concurrency are essential, such as in web servers or applications handling numerous concurrent tasks. Tokio's adept handling of multi-threaded 
asynchronous code ensures the efficient execution of tasks without compromising safety or introducing race conditions.


In summary, our decision to embrace Tokio as the asynchronous runtime for our Rust project is grounded in its status as the leading runtime in the Rust 
ecosystem, coupled with its user-friendly design and robust support for multi-threaded async code execution. This strategic choice positions us to harness the 
full potential of asynchronous programming in Rust, ensuring the responsiveness and scalability of our application.

\begin{verbatim}
#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
async fn client_test(){ 
    let ser = test_server::run_server();
    println!("Server started");
    let cl = test_client::client_test();
    println!("Client started");

    tokio::select! {
        biased; 
        _ = ser => panic!("server returned first"),
        _ = cl => (),
    }           
}
\end{verbatim}

Using Tokio makes it possible to run multiple async functions at the same time. In the example above we start a server and a client and then wait for the first one to finish. 
This is done using the tokio::select! macro. The macro takes a list of futures and waits for the first one to finish. In our case we want to wait for the client to finish 
first, so we panic if the server finishes first. If the client finishes first we just return. Any occoring Errors are cathed and returned as Result.
The two functions \verb+run_server+ and \verb+client_test+ are both async functions. The server function is a simple echo server, that waits for a message from the client and 
then sends it back. The client function sends a message containing an url and a vote to the server and then waits for the response. Server and Client are connected via a gRPC 
connection. 

\subsection{Tokio Rusqlite}
\textbf{Author: Christoph Fellner}

Tokio Rusqlite\footcite{tokiolite} stands as an innovative library at the intersection of Tokio and Rusqlite, synergizing their functionalities to enable 
asynchronous database interactions. This library represents a strategic amalgamation, providing developers with the capability to leverage Rusqlite seamlessly 
within an asynchronous programming paradigm facilitated by Tokio.

The amalgamation of Tokio and Rusqlite in Tokio Rusqlite addresses the growing demand for asynchronous database operations in Rust. By integrating Tokio's 
asynchronous runtime with Rusqlite's capabilities, this library empowers developers to perform database operations without blocking the execution of other tasks.
This is particularly advantageous in scenarios where responsiveness and concurrency are paramount.

Key features of Tokio Rusqlite include the ability to execute Rusqlite operations asynchronously, allowing for non-blocking interactions with SQLite databases. 
Asynchronous operations are vital in applications that require efficient handling of concurrent tasks, such as web servers, where multiple requests may be 
processed simultaneously.

The seamless integration of Tokio Rusqlite into our development stack augments the versatility of Rusqlite by enabling asynchronous database operations. This is 
especially valuable in scenarios where responsiveness and scalability are critical factors. Leveraging Tokio Rusqlite in our project equips us with a robust 
solution for handling database interactions in an asynchronous manner, aligning with contemporary trends in Rust programming and distributed system 
architectures.

\subsection{Tonic}
\textbf{Author: Christoph Fellner}

Tonic\footcite{tonic}, a Rust library that embodies the principles of gRPC, distinguishes itself with a keen focus on high performance, interoperability, and 
flexibility. Its seamless integration with the asynchronous paradigm, particularly its compatibility with Tokio, positions Tonic as a versatile tool for 
developing efficient and responsive distributed systems.

The synergy between Tonic and Tokio is a noteworthy feature, as Tonic is crafted to align seamlessly with Tokio's asynchronous model. This compatibility not 
only enhances the performance of asynchronous Rust code but also simplifies the integration of Tonic into existing Tokio-based projects. Leveraging async/await 
functionality, Tonic facilitates a synchronous coding style in an asynchronous environment, making it a natural fit for projects utilizing Tokio.

A distinctive aspect of Tonic lies in its support for gRPC, a high-performance remote procedure call (RPC) framework. Tonic employs Protocol Buffers to describe 
interfaces, providing a language-agnostic and efficient means of defining the structure of data. This approach not only enhances interoperability but also 
allows for the generation of necessary Rust code based on the defined Protocol Buffers, automating a significant portion of the development process.

The utilization of Tonic in our project streamlines the definition of interfaces by leveraging Protocol Buffers. This enables us to articulate our interface 
specifications in a clear and concise manner, and Tonic then takes care of the Rust code generation, reducing manual effort and potential errors.

In essence, Tonic serves as a powerful tool in our technology stack, providing a performant and flexible implementation of gRPC for Rust. Its compatibility with 
Tokio, support for async/await, and seamless integration with Protocol Buffers contribute to the development of robust and efficient distributed systems. By 
choosing Tonic, we aim to leverage its capabilities to enhance the performance, interoperability, and maintainability of our Rust-based projects.

The example for Tokio above uses the Tonic library to convert the following Protocol-Buffer-file into Rust code, in order to use it in the client and server 
functions as Service.

\begin{verbatim}
syntax = "proto3";
package voting;
        
service Voting {
    rpc Vote (VotingRequest) returns (VotingResponse);
}
        
message VotingRequest {
    string url = 1;
        
    enum Vote {
        UP = 0;
        DOWN = 1;
    }
    Vote vote = 2;    
}
        
message VotingResponse {
    string confirmation = 1;
}
\end{verbatim}

In order to translate the Protocol Buffer into Rust code we use the \verb+include_proto!+ function from the Tonic library. This function takes the path to the 
Protocol-Buffer-file and generates the Rust code for the Service.

\subsection{Docker}
\textbf{Author: Christoph Fellner}

Docker\footcite{docker} serves as an indispensable tool in modern software development, offering a containerization solution that facilitates the efficient 
execution of diverse applications within isolated environments known as containers. These containers, encapsulating applications and their dependencies, operate 
independently, enabling the concurrent execution of multiple containers on a single host system. The versatility of Docker extends from lightweight services 
like echo servers to complex web applications, making it a versatile choice for a spectrum of development and deployment scenarios.

One of Docker's compelling use cases is the encapsulation of databases within containers. This capability enables the creation of portable and reproducible 
database environments, fostering ease of testing, development, and even benchmarking. In our specific use case, Docker proved instrumental in benchmarking 
different databases, allowing us to evaluate and identify the most suitable database for our application's requirements.

Docker's open-source nature and compatibility across major operating systems contribute to its widespread adoption. Its availability on diverse platforms 
empowers developers to create consistent environments, irrespective of the underlying infrastructure. This feature is particularly advantageous in scenarios 
where multiple applications need to coexist on the same machine, as Docker mitigates compatibility concerns and ensures the isolation of applications from the 
local infrastructure.

The ability to isolate applications from the host system's infrastructure is a key feature of Docker. This isolation not only enhances compatibility but also 
simplifies the deployment process. Developers can confidently run multiple applications on a single machine without the fear of conflicts or compatibility 
issues, streamlining the development and testing phases.

In summary, Docker's role in our project extends beyond conventional application development and deployment; it serves as a pivotal tool for benchmarking 
databases. Its containerization approach, coupled with the ability to create reproducible environments, positions Docker as a valuable asset in our quest to 
identify the optimal database solution for our specific use case. The open-source nature and cross-platform compatibility further solidify Docker's standing as 
a cornerstone technology in contemporary software development and deployment practices.

\filbreak