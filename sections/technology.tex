\chapter{Technology}


\section{Wombat}

\section{Python}

\section{C++}
\textbf{Author: Maximilian Dragosits}

C++ is a high-level precompiled programming language with support for low level memory management, object oriented programming, generic and functional programming.
It was designed and created by Danish computer scientist Bjarne Stroustrup with efficiency, performance and flexibility as its core goals.\footcite{lecture_essence_cpp}
Being based on C, and due to its highly sophisticated design C++ is being used in all kinds of places nowadays. From desktop applications, servers, video games and 
even very performance critical use cases like digital equipment in space. The International Organization for Standardization (ISO) standardized this programming
language in 1998 as SO/IEC 14882:1998. Due to its popularity many libraries and frameworks for C++ have been made including Catch2\footcite{catch2_git} and 
Doxygen\footcite{doxygen_main_site} both of which are being used in this project. We chose this coding language as one of the two frontend because of its speed and
efficiency as well as the sheer number of external frameworks and libraries already created for it, which are essential in the development of the RECT library. 

\section{Rust}
\textbf{Author: Jeremy Sztavinovszki}

Rust is a general purpose multi paradigm programming language used in many fields ranging from embedded programming to web development. Although it is a relatively young language, having released its version 1.0 on May 15th 2015, it has seen great adoption from developers and has a big community. The language tries to be as fast as possibly, while still remaining memory-safe, which it achieves using its borrow checker. Even though it is possible to write unsafe code in Rust, that is not checked by the borrow checker, it is custom to keep the unsafe parts as small as possible.
Rust has a great ecosystem driven by the Rust Foundation and the Rust community. There are many tools, such as cargo, or rust-gdb, that provide great developer experience.
Right now there is now standardized async-runtime, so you normally use runtimes like tokio, async-std, or smol for programming asynchronously.

\section{gRPC}
\textbf{Author: Maximilian Dragosits}

gRPC\footcite{grpc_main_site} is an open source framework, that facilitates Remote Procedure Calls (RPC) across a multitude of environments. It has a wide variety of use cases in terms of
service to service connections and usage in the development of microservices and libraries. The framework is available in 11 different programming languages and 
has a simple service definition and generation structure in order to streamline the process of integration. It also has pluggable authentication, load balancing, tracing
and health checking in order to control service communication. gRPCs ability to connect a client to backend services is particularly important for this project 
and is therefore used to handle the communication from the individual Python and C++ frontends to the Rust backend and vice versa.

\section{Protocol Buffers}
\textbf{Author: Maximilian Dragosits}

Protocol Buffers are a platform neutral way to serialize structured data similar to XML, JSON or YAML. These data constructs can easily be used by automatically
generated source code in a multitude of different programming languages of the developers choosing. Including, but not limited to, Java, Kotlin, Python and various
C-based languages. An example of a Protocol Buffer file is this:

\begin{verbatim}
    syntax = "proto3";
    package msg;
    
    
    message From {
      string conn_name = 1;
      string topic = 2;
    }
    
    
    message To {
      string conn_name = 1;
      string topic = 2;
    }
    
    
    message Msg {
      bytes data = 1;
      oneof fromto {
        From f = 2;
        To t = 3;
      }
    }
\end{verbatim}

The first part of any .proto file is the definition of the protobuf language version. Either \textit{proto2} or \textit{proto3}. Next the package within this will be 
stored in when it is converted into a programming languages code is defined. In this case it will be \textit{msg}. After that you can import any other .proto file.
Then it is possible to define any amount of the following types and many others not used by this project:
\begin{enumerate}
    \item \textbf{message:} Defines a special data structure that houses multiple variables of potentialy different data types, which can then be used in other enums or services.
    \item \textbf{enum:} Defines an enum which acts like the equivalent type of structure in other programming languages. This can then be used in other parts of then .proto file.
    \item \textbf{service:} Defines a Remot Procedure Call (RPC) system. The generated code for this will include service interfaces and stubs to be used by RPC frameworks.
\end{enumerate}

\subsection{Protofile message definition}

Message types in proto3 are relatively simple to define.

\begin{verbatim}
    message message_name {
        field_type field_name = number;
      }
\end{verbatim}

First the \textit{message} keyword is used to signify that the following is a declaration for a message type. Then a freely choose able \textit{message\_name} is 
used as the name for the later resulting message structure. After that any number of fields can be defined within the curly brackets. The \textit{field\_type} can be
one of multiple supported data types, which includes but is not limited to double, flout, integer, boolean, string as well as bytes. After defining an appropriate
\textit{field\_name} this format requires the assignment of a number between 1 and 536,870,911 to each field in a message. This is required in order to identify
the field after encoding.

There are also three other modifiers, that can be applied to fields:

\begin{enumerate}
    \item \textbf{optional:} If a field with this modifier does not have its value explicitly set later it will instead return a default value. It also possible to check if this it has been set.
    \item \textbf{repeated:} A field with this modifier can be repeated any number of times within the message and the order of the repetition will be saved.
    \item \textbf{map:} A field with this modifier acts like a key/value pair with the definition syntax being like that of a C++ map.
\end{enumerate}

Another way of defining fields, that can have a multitude or a currently unkown type, is to use either the \textit{any} or the \textit{oneof} types.
The \textit{any} type is then later resolved by Protobufs internal reflection code.
\textit{Oneof} is then automatically later defined as one of the given data types within curly brackets placed after the \textit{field\_name} is given.

\subsection{Protofile enum definition}

Enums are share a lot of the same traits as message types in terms of the defintion syntax.

\begin{verbatim}
    enum enum_name {
        constant_value = number;
    }
\end{verbatim}

Similarly to messages the enum is given an \textit{enum\_name} and then any number of \textit{constant\_value}s can be defined. All of these constants need an associated
value in order to function properly and the first of those needs to have 0 as its number, so that the enum has a default value in cases like fields with the \textit{optional}
modifier. 
In order to bind multiple \textit{constant\_value}s to the same \textit{number} the \textit{allow\_alias} option must be set to true. This is done by inserting this line
into the enum before any definition of \textit{constant\_value}s:

\begin{verbatim}
    option allow_alias = true;
\end{verbatim}

Once an enum is defined then it can be used in other parts of the Protocol Buffer, as seen in this example:

\begin{verbatim}
    enum Success {
        Ok = 0;
    }

    enum SendError {
        NoSuchConnection = 0;
        SendFailed = 1;
    }

    message SendResponse {
        oneof result {
            Success s = 1;
            SendError err = 2;
        }
    }
\end{verbatim}

\subsection{Protofile service definition}

Services allow the easy generation of service interfaces and stubs to then be used by RPC implementations.

\begin{verbatim}
    service service_name{
        rpc rpc_name(message_type) returns (message_type) {}
        rpc rpc_name(message_type) returns (stream message_type) {}
    }
\end{verbatim}

A service is defined with a \textit{service\_name} and after that any number of inidvidual methods. In order to define the methods first the keyword \textit{rpc} must be used.
Then a name for the method is given through \textit{rpc\_name} and a parameter for the \textit{message\_type} that this method accepts. And then a \textit{message\_type}
is defined as the return value of the RPC. A stream of a particluar \textit{message\_type} can be defined by putting the keyword \textit{stream} before the type.

An example of this would be the SubListen service from this project:

\begin{verbatim}
    service SubListen{
        rpc listen(ListenRequest) returns (ListenResult) {}
        rpc subscribe(ListenRequest) returns (stream ListenResult) {}
    }
\end{verbatim}

\section{Nix}
\textbf{Author: Jeremy Sztavinovszki}

Nix is one of a couple of things depending on the context. It is either a configuration language, a package manager, an operating system, or a build system.
That may seem a bit confusing, but the next section will cover each of these contexts for the sake of clearing up some of the confusion, that may arise from this statement.

\subsection{History}
Nix was first conceived and made a reality in 2003 as a research project by Eelco Dolstra. At first it was just a package manager, that could be run on any distro, but in 2007 it became its
own full blown Linux distribution with many other additions to the Nix eco system, such as Hydra \footcite{hydra}, a continuous intergration tool, and nixops,
a deployment tool for nixos deploying NixOS in a network/cloud \footcite{NixOps}. At the time of writing Nix has gotten another big addition in the form of flakes.
Flakes are a way of declaratively building anything ranging from Nix packages, over development environments, to system configurations. When a flake is build for the first time it
pulls in all of its inputs and writes their commit hashes to a flake.lock file. Through this mechanism of noting the exact commit each input of a flake it is possible to use nix flakes for
reproducible builds.

\subsection{The Language}
The language was the first part of Nix, that was implemented and it is arguably the most important part of Nix. Nix is a declarative functional programming language,
that is used for defining packages, build processes and configurations for a host of things ranging from reproducible development environments to IT-Infrastructure.
Taking an example for the syntax from the official nixos wiki site the language looks something like this:

% example here
\begin{minipage}{\textwidth}
\begin{lstlisting}[language=Nix, caption={Simple Examples of the Nix Language}]
#1
let
    a = 1;
    b = 2;
in a+b
# result 3

#2
let
    square = x: x*x;
    a = 12;
in square a
#result 144

#3
let 
    add = x: y: x+y;
in add 1 2

#result 3

#4
let
    square = {x ? 2}: x*x;
    a = 12;
in square{x=square {};}

#result 16
\end{lstlisting}
\end{minipage}

%description of the example here

In examining the provided code, a discernible pattern emerges. The syntax \newline \verb+let <variables> in <statement>+ serves as a method for defining values within a forthcoming block and it is quite reminicent of the Haskell programming language. However, this construct encompasses more than mere value assignment.
Consideration of each distinct block sheds light on its functionality. Block No. 1 initializes two variables, 'a' and 'b', then performs an addition operation on them. It's noteworthy that if an additional variable 'c' were defined but left unused, it would remain unevaluated due to Nix's lazy evaluation mechanism.
Moving to Block No. 2, it defines a function named 'square'. This function accepts a parameter 'x', as indicated by the notation \verb+x: x*x+. Parameter declarations in this context follow the structure \verb+<parameter name>: <statement>+. However, when multiple variables are required, as demonstrated in Block No. 3, the syntax adapts to \verb+<parameter name 1>:<parameter name 2>: <statement>+. This paradigm bears resemblance to lambda calculus \footcite{lambda_calculus}.
Block No. 4 showcases optional parameters. This feature is denoted by \newline \verb+{<parameter name> ? <default value>}: <statement>+.


\subsection{The Package Manager}
Nix, now a package manager, is a cross-platform package manager, that claims to have solved a problem called dependency hell \footcite{dependency_hell}, by keeping track of which package needs which dependencies. If a package is no longer needed it can automatically be garbage collected. Packages are installed to a directory called the nix store and have a unique hash, which is generated by combining some factors, like dependencies, versions and so on.
When defining a package you use the nix programming language and lazy functional programming to declare how to build it, what you need to build it and what files to install through a format called derivations.

\subsection{The Build System}
The following example showcases how to use the Nix language to define a flake containing a package, which can be built and installed on any system running the Nix package manager.
The specific example builds the latex files of which this thesis is comprised into a pdf file using a build tool called latexmk.
\medskip

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=Nix, caption={The nix flake, that builds this diploma thesis}]
{
  description = "A flake to build the RECT-Diploma-Thesis";

  inputs = {
    nixpkgs.url ="github:nixos/nixpkgs/nixos-23.05";
  };

  outputs = {self, nixpkgs}:
  let
    system = "x86_64-linux";
    pkgs = nixpkgs.legacyPackages.${system};
  in {
		packages.${system}.default = pkgs.stdenv.mkDerivation rec {
			name = "RECT-Diploma-Thesis";

			src = ./.;

			buildInputs = [
				pkgs.texlive.combined.scheme-full
			];

			buildPhase = ''
					latexmk -pdf main.tex
			'';

			installPhase = '' 
					mkdir -p $out/pdf
					cp main.pdf $out/pdf
			'';
		};
  };
}
\end{lstlisting}
\end{minipage}

The code shown above is a nix flake. It defines the nixpkgs repository as an input and the result of the build process,
that is taking place in the mkDerivation block as an output. mkDerivation is a function which takes a name, pname, version, src,
buildInputs, buildPhase, installPhase, builder and shellHook as inputs and produces a package which is built in the standard environment (stdenv) \footcite{nixMkDerivation}.
The built derivation (or output of this flake) is then asigned a hash and stored in the nix store on the machine that built it.
An example of this would be the following path \newline\verb+/nix/store/dnx26izplgv46dwg548whh9kj5iz4vvx-RECT-Diploma-Thesis+. 

\subsubsection{Nixpkgs}
Of course the defined packages need to be stored somewhere. This is where the nixpkgs repository \footcite{nixpkgs_repo} on github comes in handy. It is a collection of over 80000 packages according to repology \footcite{repology_nixpkgs}.
The community can contribute their own definitions, or updates to the repository if they found something to be out of date, or missing. Of course when installing a package it is not built from scratch every time, like on source based distros.
Instead nixpkgs caches builds of the most popular packages, which then just have to be downloaded onto the users machines.

\subsection{The Operating System}
NixOS is built upon the Nix package manager. It is an independent Linux distribution, which means it is not based on any other Linux distribution like for example Debian, or Arch Linux.
What is really special about NixOS is, that the whole operating system with services, programs and all of the needed configurations can be built from one central file, which is written in the Nix Programming Language.
With this file stored as a backup a machine running NixOS could be up and running again in no time after a failure and through the usage of Nix Flakes it is possible to reinstall the system exactly the way it was before.

\section{Bluetooth Low Energy}
\textbf{Author: Jeremy Sztavinovszki}

\subsection{BLE Layers}
\subsubsection{Protocol vs. Profile}
The BLE specification has two important concepts, which it has clearly separated since its inception. These concepts are profiles and protocols. Hereby protocol is used to describe
the basic parts upon which the BLE stack builds and can be thought of the horizontal layers of the stack. Profile refers to vertical slices of the stack, that are used for specific use-cases when developing with BLE. Examples of a profile are.

\begin{itemize}
	\item The Glucose profile, which is used to securely transmit measurement data from e.g. insulin pumps.
	\item The Find Me profile, which allows devices to find one another
\end{itemize}

These use specific parts of the BLE protocol to achieve a specific task.

\subsubsection{Physical Layer PHY}
The Physical Layer (PHY) establishes the foundation for BLE communication by defining radio transmission properties such as modulation schemes, frequency bands, and power levels. It optimizes communication for BLE devices by employing techniques like frequency-hopping spread spectrum (FHSS) to mitigate interference and enhance reliability. Advancements in the PHY layer aim to improve data rates, extend range, and enhance spectral efficiency while maintaining low power consumption, which is crucial for the versatility of BLE applications.

\subsubsection{Link Layer (LL)}
The Link Layer(LL)
is responsible for managing essential functions such as connection establishment, maintenance, and data transmission between BLE devices. It handles advertising, scanning, and packet acknowledgment, optimizing power usage during data exchange. The efficient handling of packet formatting, acknowledgment, and error handling ensures a robust and reliable communication link, which is pivotal for BLE's energy-efficient operations.

\subsubsection{Host Controller Interface (HCI)}
The Host Controller Interface (HCI)
serves as the intermediary between the host (application processor) and Bluetooth hardware on the host side. It defines protocols and commands for seamless data exchange, enabling efficient control of Bluetooth functionalities.
The details of the host side are also included.

\paragraph{Host Side}
The HCI on the host side enables communication between the HCI driver and the application processor. It offers a standardised interface that defines the command structures and protocols used by the application to interact with the Bluetooth hardware. This abstraction allows for platform-independent communication and streamlines application development.
The Controller Side should also be considered.

\paragraph{Controller Side}
The HCI on the controller side translates commands from the host into hardware-specific operations for the Bluetooth controller. It facilitates communication between the host and the Bluetooth hardware, ensuring accurate execution of the required actions. This layer is responsible for managing data transfer between the host and controller, optimizing the transmission process


\subsubsection{Logical Link Controll and Adaptation Protocol (L2CAP)}
The Logical Link Control and Adaptation Protocol (L2CAP) efficiently multiplexes higher-layer protocols over BLE connections. It segments and reassembles data packets, optimizing data transmission efficiency while accommodating diverse application requirements. The role of protocol multiplexing and fragmentation is to ensure efficient data exchange while maintaining BLE's low-energy characteristics.

\subsubsection{Attribute Protocol and Generic Attribute Profile}
The Attribute Protocol (ATT) and Generic Attribute Profile (GATT) play critical roles in defining how data is exchanged and accessed between devices. ATT outlines rules for attribute information exchange, while GATT specifies the structure and mechanisms for accessing these attributes. The structured approach provided by data representation and interaction ensures interoperability across various applications and device types.

\subsubsection{Security Manager (SM)}
The Security Manager (SM) operates within the BLE protocol stack and is responsible for establishing secure connections and managing security-related aspects between BLE devices. It manages processes such as pairing, encryption, and authentication to ensure the confidentiality and integrity of data transmitted over BLE connections. This is critical for safeguarding sensitive information.

\subsubsection{General Access Profile}
The Generic Access Profile (GAP) is a fundamental layer in Bluetooth Low Energy (BLE) that is responsible for device discovery, connection setup, and addressing within the network. It defines device roles such as Peripheral and Central, managing how devices interact. Peripherals broadcast their presence, allowing Centrals to discover and connect. GAP also handles device addressing, ensuring unique identification, and manages visibility, pairing, and power modes. GAP promotes interoperability between devices by standardizing essential functions. This ensures smooth communication across diverse BLE devices, regardless of their manufacturers or applications. The layer's importance lies in its ability to provide stability and reliability in BLE networks.

\subsection{Network Topologies}
Using the above protocols and layers, there is a wide range of network topologies that designers can choose from, depending on their objectives.
Some of the following topologies excel at saving power, while others are great for scalability and redundancy.

\subsubsection{Star Topology}
In a star topology, a central device (the 'Central') communicates with multiple peripheral devices. This configuration is similar to a hub-and-spoke model, where the central device manages and controls communication with the peripherals. For example, in a smart home scenario, a smartphone (acting as the central) connects to various peripherals such as smart locks, lights or sensors. The central device collects data from these peripherals and can also coordinate their functions. This architecture is simple and provides centralised control, but it relies heavily on the availability and range limitations of the central device.

\subsubsection{Mesh Topology}
BLE mesh networks allow devices to communicate with each other, forming a decentralised network without a central control point. Each device, or 'node', can communicate with nearby nodes, extending the coverage and redundancy of the network. For example, in a smart lighting system, each bulb can communicate with neighbouring bulbs to relay commands or data, ensuring robustness even if a node fails. Mesh networks are scalable, resilient and suitable for applications that require extensive coverage, such as smart buildings or large-scale sensor networks.

\subsubsection{Hub and Spoke Topology}
This architecture involves a central 'hub' device that communicates with multiple 'spoke' devices, each of which communicates only with the central hub. Think of a smart home setup where a central controller (e.g. a smart hub or gateway) manages and interacts with various sensors, smart appliances or actuators placed throughout the home. Each peripheral device communicates only with the central hub, streamlining communication and enabling centralised management.

\subsubsection{Cluster Tree Topology}
The cluster tree topology forms a hierarchical structure that organises devices into clusters, each cluster having a central node. These central nodes can communicate with other central nodes or higher level devices, creating a structured network. In industrial applications, devices within a particular area or zone can communicate with a local coordinator, which then communicates with higher level coordinators or a central system. This hierarchy enables efficient communication in large deployments, providing local and global control.

\subsubsection{Hybrid Architectures}
BLE applications often use hybrid architectures that combine multiple topologies to meet different needs. For example, a smart building might use a mesh network for inter-floor sensor communication, a star network for room-level control (with each room having a central controller), and a hub-and-spoke network for centralised management and integration of various systems within the building.

\section{Libraries}

\subsection{Catch2}
\textbf{Author: Maximilian Dragosits}

Catch2\footcite{catch2_main_site} is a C++-based unit testing framework. It is design to be easily integrated into C++ code and match the overall look and feel
of normal functions and boolean expressions. This framework also provides micro-benchmarking capabilities. It is a good fit for this project, because it serves
to develop the C++ frontend with unit tests in mind and optimizations spurred on by benchmarks measuring the speed and efficiency of the implemented methods.

\subsubsection{Unit Tests}

Unit Tests in Catch2 are defined very similarly as normal functions in C++. This example, pulled from the Github repository of Catch2, shows the simplicity of
this framework and its integration into projects.

\begin{verbatim}
    #include <catch2/catch_test_macros.hpp>

    #include <cstdint>
    
    uint32_t factorial( uint32_t number ) {
        return number <= 1 ? number : factorial(number-1) * number;
    }
    
    TEST_CASE( "Factorials are computed", "[factorial]" ) {
        REQUIRE( factorial( 1) == 1 );
        REQUIRE( factorial( 2) == 2 );
        REQUIRE( factorial( 3) == 6 );
        REQUIRE( factorial(10) == 3'628'800 );
    }
\end{verbatim}

First the relevant Catch2 headers are included and then a function with a return value is defined. In this case it is the function factorial. 
This function will be executed by Catch2 during the testing process. Then a test case is a macro defined as:

\begin{verbatim}
TEST_CASE(string testname, string tags) {...test...}
\end{verbatim}

The argument \textit{testname} is a arbitrary name given to the unit test, which is then later during the running of the test printed alongside the results of the macro.
Tags can be given to the test by inputting one or multiple tags into the \textit{tags} field and change the behavior of the macro accordingly. In the case of the example above
the only given tag is the name of the function to be tested. After this the \textit{TEST\_CASE} macro has a curly brackets-enclosed body in which the logic of the test can 
be defined.
This requires the use of other specific macros included in the Catch2 framework. For example:

\begin{verbatim}
REQUIRE( function(value) == expected_value );
CHECK( function(value) == expected_value );
\end{verbatim}

The two macros described above, REQUIRE and CHECK, operate in a similar way. They both execute the given \textit{function} with the provided \textit{value} or \textit{values}
and then assert if the returned data equals true or false according to the provided boolean operator. If it does then it was successful and the rest of the \textit{TEST\_CASE} is executed. The difference 
between REQUIRE and CHECK is however that if a REQUIRE macro fails it throws an exception and the unit test is stopped from executing the remainder of code inside it.

\subsubsection{Micro-benchmarks}
The benchmarking macros in Catch2 are defined similarly to how unit tests are. Benchmarking in itself is a useful practice, that provides a way to measure the performance
and speed of a particular function.

\begin{verbatim}
#include <catch2/catch_test_macros.hpp>
#include <catch2/benchmark/catch_benchmark.hpp>

#include <cstdint>

uint64_t fibonacci(uint64_t number) {
    return number < 2 ? number : fibonacci(number - 1) + fibonacci(number - 2);
}

TEST_CASE("Benchmark Fibonacci", "[!benchmark]") {
    REQUIRE(fibonacci(5) == 5);

    REQUIRE(fibonacci(20) == 6'765);
    BENCHMARK("fibonacci 20") {
        return fibonacci(20);
    };

    REQUIRE(fibonacci(25) == 75'025);
    BENCHMARK("fibonacci 25") {
        return fibonacci(25);
    };
}
\end{verbatim}

In the example above the unit test macros and benchmark macros from Catch2 are first included in order to be used later. The function to be benchmarked is then defined
After that the TEST\_CASE macro is used combined with the [\!benchmark] tag in order to turn this unit test into a benchmark. In the actual body of the test the function
is first tested weather or not it actually works as intended before any benchmarks are done. This is done with the REQUIRE macro, since it throws an exception if the 
assertion fails, preventing the rest of the benchmark from executing unnecessarily. If all the tests before the benchmarks pass then the actual BENCHMARK macros are
executed.

\begin{verbatim}
BENCHMARK(string name) {
    ... benchmark ...
};
\end{verbatim}

As part of the BENCHMARK macro a arbitrary name is given to it, which is then later during the output of the test used as a identifier for the specific benchmark.
Then the actual logic of the benchmark is then defined within curly brackets giving a lot of freedom in how a certain benchmark is executed. Adding a return statement
within the benchmark will ensure that the compiler doesn't mess with the test.

After this is run a summary is automatically output within the command line window. This includes multiple data points that pertain to the execution speed of the tested
function:
\begin{enumerate}
    \item \textbf{Samples:} The amount of times the code within the curly brackets of the BENCHMARK macro is repeated in order to build a dataset to calculate the mean execution time.
    \item \textbf{Iterations:} %Need to find out what this is
    \item \textbf{Estimated run time:} The estimated amount of time the code within the benchmark will take to run. Mesured in milliseconds.
    \item \textbf{Mean/low mean/high mean run time:} The mean time it will take for the code to run as well as the low mean and high mean for this in nanoseconds.
    \item \textbf{Standard deviation:} The standard deviation from the mean time in nanoseconds,
\end{enumerate}

\subsection{Doxygen}
\textbf{Author: Maximilian Dragosits}
tbd
%Explain what Doxygen is

\subsubsection{Documentation}
%Explain how to document with Doxygen

\subsection{Rusqlite}
\textbf{Author: Christoph Fellner}

Rusqlite is an ergonomic wrapper for using SQLite from Rust similar to rust-postgres. It provides an easy-to-use interface to work with SQLite databases. Using the functions provided by rusqlite you can perform all the common database operations, such as creating tables, inserting Data, querying data, and more simply within your code.
We choose this library for SQLite mainly because of three reasons:
\begin{enumerate}
	\item \textbf{Portability:} SQLite databases can be used across different platforms and operating systems. Given the fact that RECT operates on different small Controllers, portability is very important.
	\item \textbf{Configuration:} There is no need for any complex setup or configuration when working with SQLite. We only need a few tables to work with, so having to configure a database on each controller wouldn't be worth the effort.
	\item \textbf{Local:} SQLite doesn't need any separate server or installation. It contains all the features we need in a small an independent package.
\end{enumerate}

In our case we use rusqlite in order to save the config.json file, containing date for the available connections. Accessing the Data from the database is simply much quicker and saver than reading from the file when we need it.

\subsection{Serde}
\textbf{Author: Christoph Fellner}

Serde is a framework for rust, used for \textbf{ser}ializing and \textbf{de}serializing data structures efficiently and generically. You can find a detailed serde overview \href{https://serde.rs/}{here}.

Serde provides functions to deserialize JSON-files in a simple and quick way, this allows us to use the data from the config.json file in our program with just a few lines of code.

With serde we deserialize the data from the JSON-file into a self-made rust structure, which allows us to use the data properly.

\subsection{Tokio}
\textbf{Author: Christoph Fellner}

Tokio is asynchronous runtime for rust. In rust asynchronous code doesn't run on it's own in order to make it work the programmer has to use a runtime like Tokio. You can find more in depth descripton of Tokio \href{https://tokio.rs/tokio/tutorial}{here}.

We picked Tokio because it is the most widely used runtime for async rust code. There are also a lot of Tutorials for Tokio and it is fairly simple to use.

Tokio as our asynchronous runtime allows us to execute multi-threaded async code safely.

\subsection{Tonic}
\textbf{Author: Christoph Fellner}

Tonic is a rust framework that implements gRPC.

\filbreak
